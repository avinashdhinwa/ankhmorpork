apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: thaum-rules
  namespace: monitoring
spec:
  groups:
    - name: custom node alert rules
      rules:
      - alert: PackagesAvailable
        expr: 'sum(yum_upgrades_pending{instance="nas.ankhmorpork.thaum.xyz:9100"}) > 10'
        for: 24h
        labels:
          severity: warning
        annotations:
          summary: "System is ready for upgrade"
          description: "More than 10 packages is available for upgrade. Maybe it is time to upgrade?"
      - alert: RAIDCriticalFailure
        expr: 'node_md_disks_required - ignoring (state) (node_md_disks{state="active"}) != 0'
        for: 15m
        labels:
          severity: critical
          priority: P1
        annotations:
          summary: "Degraded RAID array on {{ $labels.instance }}"
          description: "RAID array '{{ $labels.device }}' is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically."
      - alert: RAIDDiskFailure
        expr: 'node_md_disks{state="fail"} > 0'
        labels:
          severity: warning
        annotations:
          summary: "Failed device in RAID array on {{ $labels.instance }}"
          description: "At least one device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.md_device }}' needs attention and possibly a disk swap"
      - expr: abs(node_timex_offset_seconds)
        record: instance:node_timex_offset:abs
      - alert: Watchdog
        expr: vector(1)
        for: 10m
        labels:
          severity: warning
        annotations:
          description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.'
          summary: 'Ensure entire alerting pipeline is functional'
      - alert: RebootRequired
        expr: "node_reboot_required > 0"
        labels:
          severity: info
        annotations:
          message: "Instance '{{ $labels.instance }}' was upgraded and now requires a reboot."
      - alert: FilesystemReadOnly
        expr: |
          (node_filesystem_device_error{fstype!~"(tmpfs|proc|aufs|rpc_pipefs|nsfs|overlay|squashfs)"} != 0)
          and
          (node_filesystem_readonly{fstype!~"(tmpfs|proc|aufs|rpc_pipefs|nsfs|overlay|squashfs)"} != 0)
        labels:
          severity: info
        annotations:
          message: "Filesystem went read-only due to device error"
    - name: alert rules specific to thaum.xyz backups
      rules:
      - alert: BackupNotCreated
        expr: 'increase(backup_executions_total[48h]) < 1'
        for: 12h
        labels:
          severity: warning
        annotations:
          description: "Backuping up {{ $labels.data }} on {{ $labels.instance }} didn't complete"
          summary: "Backup not created"
      - alert: BackupMissing
        expr: 'time() - backup_end_last_timestamp > 8 * 24 * 60 * 60'
        labels:
          severity: warning
        annotations:
          message: "Missing backup of {{ $labels.data }} on {{ $labels.instance }} in {{ $labels.namespace }} for over 8 days"
    - name: alert rules specific to thaum.xyz environments
      rules:
      - alert: FederatedPrometheusDown
        expr: 'up{job="lancre"} == 0'
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Remote Prometheus server {{ $labels.instance }} has been down for more than 5 minutes."
          summary: "Federated Prometheus down on {{ $labels.instance }}"
